)
fit_high <- glm(
claim_count ~ offset(log(revmillions)) + discipline_factor,
data = pol_low,
family = quasipoisson()
)
summary(fit_low)
summary(fit_high)
fit_high <- glm(
claim_count ~ offset(log(revmillions)) + discipline_factor,
data = pol_high,
family = quasipoisson()
)
summary(fit_low)
summary(fit_high)
copy.model(fit_low)
copy.model(fit_high)
str(policies)
#' ## Data Checks
#'
#' Make sure that column types are as expected.
str(polcies)
#' ## Data Checks
#'
#' Make sure that column types are as expected.
str(policies)
tpol <- tree(
claim_count ~ revenue + discipline +
state_group + employee_count + five_year_claims +
year_started + use_written_contracts, policies)
tplo
tpol
plot(tpol)
text(tpol)
tpol <- tree(
claim_count ~ revenue + discipline +
state_group + employee_count +
year_started + use_written_contracts, policies)
plot(tpol)
text(tpol)
tpol <- tree(
claim_count ~ revenue + discipline +
state_group +
year_started + use_written_contracts, policies)
tpol <- tree(
claim_count ~ revenue + discipline +
state_group +
year_started, policies)
tpol <- tree(
claim_count ~ discipline +
state_group +
year_started, policies)
tpol <- tree(
claim_count ~  +
year_started, policies)
discipline +
state_group
tpol <- tree(
claim_count ~ discipline +
state_group +
year_started, policies)
tpol <- tree(
claim_count ~ revenue, policies)
plot(tpol)
text(tpol)
?tree
tpol <- tree(claim_count ~ revenue, policies)
tpol <- tree(claim_count ~ revenue + discipline_factor, policies)
plot(tpol)
text(tpol)
tpol <- tree(claim_count ~ revenue + discipline_factor + state_group_factor, policies)
plot(tpol)
text(tpol)
str(policies)
tpol <- tree(
claim_count ~ revmillions + discipline_factor +
state_group_factor + year_started +
employee_count + use_written_contracts,
policies
)
tpol <- tree(
claim_count ~ revmillions + discipline_factor +
state_group_factor + year_started +
employee_count + use_written_contracts_factor,
policies
)
policies$use_written_contracts_factor <- as.factor(use_written_contracts)
policies$use_written_contracts_factor <- as.factor(policies$use_written_contracts)
tpol <- tree(
claim_count ~ revmillions + discipline_factor +
state_group_factor + year_started +
employee_count + use_written_contracts_factor,
policies
)
plot(tpol)
text(tpol)
pol_low <- policies[policies$revenue < 4e6, ]
pol_high <- policies[policies$revenue >= 4e6, ]
nrow(pol_low)
nrow(pol_high)
#' I expect this to be the correct frequency model!
fit_low <- glm(
claim_count ~ offset(log(revmillions)) + state_group_factor + discipline_factor,
data = pol_low,
family = quasipoisson()
)
fit_high <- glm(
claim_count ~ offset(log(revmillions)) + discipline_factor,
data = pol_high,
family = quasipoisson()
)
summary(fit_low)
summary(fit_high)
copy.model(fit_low)
copy.model(fit_high)
#' ---
#' title:   "Check fitting methods against chosen parameters"
#' author:  "Adam Rich"
#' date:    "July 26, 2018"
#' output:  pdf_document
#' ---
#' ## Prep Stuff
#'
#' These objects were created at the same time as the
#' smaller problem datasets.
#' Also load required packages.
(load('./data/created-data-full.RData'))
source('./R/resources.R')
require(tidyverse)
require(actuar)
require(MASS)
require(fitdistrplus)
require(tree)
#' ## Purpose
#'
#' The purpose of this R file is to
#' try out the fitting methods I expect will work
#' and show that they provide something very close
#' to the pre-selected frequency and severity models.
#'
#' ## Data Checks
#'
#' Make sure that column types are as expected.
str(policies)
#' ## Pre-chosen Relativites and Parameters
#'
#' State relativities are
#'
#'   * Low = 1.0
#'   * Med = 1.25
#'   * High 1.5
#'   * For companies with revenue over $4m, override with 3.5
#'
#' Discipline relativities are supposed to be:
disciplines[, c('Discipline', 'Relativity')]
#' The base frequency is 0.05 claims per million in revenue.
#' ## Severity Curve
#'
#' I'll start with the severity curve since it is
#' probably easier than the frequency.
#' I know that the claims come from a lognormal distribution
#' with the same parameters, regardless of any policy
#' characteristics.
#'
#' An effective way to know whether data is from a
#' lognormal distribution is to do a qqnorm plot
#' on the log values.
qqnorm(log(losses$claim_ultimate))
#' The moments of the sample data are
mean_loss <- mean(losses$claim_ultimate)
sd_loss <- sd(losses$claim_ultimate)
mean_loss
sd_loss
#' ## Test a Gamma
#'
#' A gamma distribution with the same moments would have these parameters
#' (k is shape, theta is scale):
k <- (mean_loss / sd_loss)^2
theta <- mean_loss / k
k * theta
sqrt(k) * theta
losses_gamma <- rgamma(
n = nrow(losses),
shape = k,
scale = theta
)
mean(losses_gamma)
sd(losses_gamma)
#' The `qqnorm` plot for this just doesn't fit a straight line.
qqnorm(log(losses_gamma))
#' ## Severity Parameters
#'
#' The actual lognormal parameters used are
(p <- unique(losses[, c('sev_mu', 'sev_sigma')]))
mu_actual <- p[, 1]
sigma_actual <- p[, 2]
#' Using the `fitdistrplus` package,
#' we get these fitted parameters:
(b <- fitdistrplus::fitdist(losses$claim_ultimate, 'lnorm'))
mu_fitted <- b$estimate[1] %>% unname
sigma_fitted <- b$estimate[2] %>% unname
#' A function for lognormal limited expected value is
levlnorm <- function(x, mu, sigma) {
exp(mu + sigma^2/2) *
pnorm((log(x) - mu - sigma^2) / sigma) +
x * (1 - plnorm(x, mu, sigma))
}
#' The expected values, unlimited and then limited at
#' $1m, $5m, and $10m are:
actual_U <- exp(mu_actual + sigma_actual^2/2)
actual_1 <- levlnorm(1e6, mu_actual, sigma_actual)
actual_2 <- levlnorm(2e6, mu_actual, sigma_actual)
actual_5 <- levlnorm(3e6, mu_actual, sigma_actual)
#' Check using "brute force"
integrate(
lower = 1,
upper = 100e6,
f = function(t) {t * dlnorm(t, mu_actual, sigma_actual)}
)
integrate(
lower = 1,
upper = 100e6,
f = function(t) {pmin(t, 1e6) * dlnorm(t, mu_actual, sigma_actual)}
)
integrate(
lower = 1,
upper = 100e6,
f = function(t) {pmin(t, 2e6) * dlnorm(t, mu_actual, sigma_actual)}
)
integrate(
lower = 1,
upper = 100e6,
f = function(t) {pmin(t, 5e6) * dlnorm(t, mu_actual, sigma_actual)}
)
#' The severities are so low in this distribution that the limits
#' of $1m, $2m, and $5m are basically the same price.
#' ## Frequency
#'
#' There is an engineered break at $4m in revenue.  Below that,
#' revenue, state group and discipline are all predictive.
#' Above that, state group is no longer predictive.
#'
#' One thing that is really important to check before
#' doing a GLM on frequency is whether a Poisson error is
#' appropriate.
#' This check is done by comparing the mean and variance of the
#' observations.
mean(policies$claim_count)
var(policies$claim_count)
var(policies$claim_count) / mean(policies$claim_count)
#' They are *not* close so Poisson is not appropriate.
#' However, a negative binomial distribution has
#' variance greater than the mean.
# g <- glm.nb(
#   claim_count ~ revenue + state_group + discipline,
#   data = policies[policies$revenue < 4e6, ]
# )
#' The problem with the glm.nb is that it assumes that
#' theta is a constant.  But, I know that
#' var(X) = o.d.f * E[X]
#' where o.d.f. is a constant.
#' So, I need a family with a log link that allows for this linear relationship
#' between variance and mean.
#' Create some new columns in the dataset
#' Re-base the factors, too:
#'
#'   * Base state = Low group (lowest relativity)
#'   * Base discipline = Landscape Architecture (lowest relativity)
#'
policies$revmillions <- policies$revenue / 1e6
dlevels <- c(
"Landscape Architecture", "Surveyor", "Mechanical Engineering",
"Civil Engineer", "Architect", "Structural Engineer")
slevels <- c("Low", "High", "Mid")
policies$state_group_factor <- factor(
x = policies$state_group,
levels = slevels
)
policies$discipline_factor <- factor(
x = policies$discipline,
levels = dlevels
)
policies$use_written_contracts_factor <- as.factor(policies$use_written_contracts)
#' ## Engineered Break
#'
#' Not all revenue bands have the same model!
tpol <- tree(
claim_count ~ revmillions + discipline_factor +
state_group_factor + year_started +
employee_count + use_written_contracts_factor,
policies
)
plot(tpol)
text(tpol)
pol_low <- policies[policies$revenue < 4e6, ]
pol_high <- policies[policies$revenue >= 4e6, ]
nrow(pol_low)
nrow(pol_high)
#' I expect this to be the correct frequency model!
fit_low <- glm(
claim_count ~ offset(log(revmillions)) + state_group_factor + discipline_factor,
data = pol_low,
family = quasipoisson()
)
fit_high <- glm(
claim_count ~ offset(log(revmillions)) + discipline_factor,
data = pol_high,
family = quasipoisson()
)
summary(fit_low)
summary(fit_high)
copy.model(fit_low)
copy.model(fit_high)
#' ## Prep Stuff
#'
#' These objects were created at the same time as the
#' smaller problem datasets.
#' Also load required packages.
(load('c:/home/git/other/ratemaking-capstone/data/created-data-full.RData'))
#' ## Prep Stuff
#'
#' These objects were created at the same time as the
#' smaller problem datasets.
#' Also load required packages.
(load('c:/home/git/other/ratemaking-capstone/data/created-data-full.RData'))
source('c:/home/git/other/ratemaking-capstone/R/resources.R')
require(tidyverse)
source('c:/home/git/other/ratemaking-capstone/R/resources.R')
(load("c:/home/git/other/ratemaking-capstone/data/claims.RData"))
(load("c:/home/git/other/ratemaking-capstone/share/claims.RData"))
(load("c:/home/git/other/ratemaking-capstone/share/pol_dates.RData"))
(load("c:/home/git/other/ratemaking-capstone/share/pol_rating.RData"))
state_lookup <- read.csv('c:/home/git/other/ratemaking-capstone/share/states.csv', stringsAsFactors = FALSE)
head(claims)
#' ---
#' title:   "Capstone Day 1: Prepare Data"
#' author:  "Adam Rich"
#' date:    "July 25, 2018"
#' output:  pdf_document
#' ---
#' The goal for day 1 is to create two datasets
#'
#'   * `pol_final` for analyzing frequency
#'   * `claims_final` for analyzing severity
#'
#'
#' There are a few steps that have to be done
#'
#'   1. Load the four data files to memory
#'   1. Spread `pol_rating`
#'   1. Join the new wide `pol_rating` object to `pol_dates`
#'   1. Join with the state lookup table
#'   1. Put rating characteristics back in claims table
#'   1. Aggregate claims data by policy
#'   1. Join agg claims with policy data
#'   1. Add some derived columns
#'   1. Do some sense checking
#'   1. Save files
#'
#' I'll use `tidyverse` package because it automatically loads `dplyr` and `tidyr`.
#' I'll need `tidyr` to "reshape" or "spread" the `pol_rating` data object.
#' The capstone project ZIP also came with *resources.R* so let's `source` that, too.
require(tidyverse)
source('c:/home/git/other/ratemaking-capstone/R/resources.R')
#' ## Load the four data files to memory
#' Note: Putting paranthesis around a statement
#' will force that statement's return value to be printed.
#' Usually a function's return value *is* printed,
#' but some, like `load` try to be "silent".
#' Since we are writing a report, I want to see the output.
#'
#' The return value of `load` is a vector giving the names of the objects loaded.
(load("c:/home/git/other/ratemaking-capstone/share/claims.RData"))
(load("c:/home/git/other/ratemaking-capstone/share/pol_dates.RData"))
(load("c:/home/git/other/ratemaking-capstone/share/pol_rating.RData"))
state_lookup <- read.csv('c:/home/git/other/ratemaking-capstone/share/states.csv', stringsAsFactors = FALSE)
#' This is what the four data frames look like.
head(claims)
str(claims)
head(pol_dates)
str(pol_dates)
head(pol_rating)
str(pol_rating)
head(state_lookup)
str(state_lookup)
#' ## Spread `pol_rating`
pol_rating_wide <- pol_rating %>%
spread(key = variable, value = value)
head(pol_rating_wide)
str(pol_rating_wide)
#' ## Join the new wide `pol_rating` object to `pol_dates`
pol <- pol_rating_wide %>% inner_join(pol_dates)
head(pol)
str(pol)
#' ## Join with the state lookup table
#' The data frames `state_lookup` and `pol` do **not** spell "state" the same.
#' One has an uppercase 'S' the other lowercase.
#' Since R is case-sensitive, this is a problem.
#' This statement would give an error
#+ eval=FALSE
# Gives an error!
pol_state <- pol %>% inner_join(state_lookup)
#' One option is to use the `by` arg of `inner_join`.
pol_state <- pol %>%
inner_join(state_lookup, by = c("state" = "State"))
#' Another option is to rename the columns of `state_lookup` first.
#' Then do the join.
#' I like this one because I want to rename the other columns anyway.
names(state_lookup) <- c('state', 'state_group', 'state_population')
pol_state <- pol %>%
inner_join(state_lookup)
#' OK, let's do this a third time, because I actually
#' don't want `state_population` in the joined table.
state_group_lookup <- state_lookup[, c('state', 'state_group')]
pol_state <- pol %>%
inner_join(state_group_lookup)
head(pol_state)
str(pol_state)
#' ## Put rating characteristics back in claims table
#'
#' Let's do this step before aggregating the claims data
#' and adding to the policy data.
#' The reason is that I don't need "total claim count" by policy
#' added back to the claims data.
claims_final <- claims %>% inner_join(pol_state)
head(claims_final)
str(claims_final)
#' ## Aggregate claims data by policy
#'
#' Like everything in R there is more than one way to do this.
#' Who knows which is better...
# One option for aggregating claims
claims_agg <- claims %>%
group_by(policy_number) %>%
summarize(
total_ultimate = sum(claim_ultimate),
claim_count = n())
# Another option for aggregating claims
claims$count <- 1
claims_agg <- claims %>%
group_by(policy_number) %>%
summarize(
total_ultimate = sum(claim_ultimate),
claim_count = sum(count))
head(claims_agg)
str(claims_agg)
#' ## Join agg claims with policy data
#'
#' Doing the join is easy.
#' But because this is a left join,
#' Any policies without claims will have NAs in the `total_ultimate`
#' and `claim_count` columns.
#' But, this will cause problems in modeling later,
#' so we will change NAs in these columns to 0.
pol_final <- pol_state %>%
left_join(claims_agg)
#' There are different ways to replace NAs with zeros.
#' I'll use one for `total_ultimate`.
pol_final$total_ultimate[is.na(pol_final$total_ultimate)] <- 0
#' And, another for `claim_count`.
pol_final$claim_count <-
ifelse(is.na(pol_final$claim_count), 0, pol_final$claim_count)
head(pol_final)
str(pol_final)
#' ## Add some derived columns
# I want this claim in both!
pol_final$years_in_business <-
year_yyyymm(pol_final$inception) - as.integer(pol_final$year_started) + 1
claims_final$years_in_business <-
year_yyyymm(claims_final$inception) - as.integer(claims_final$year_started) + 1
pol_final$average_severity <- ifelse(
pol_final$claim_count == 0,
0,
pol_final$total_ultimate / pol_final$claim_count
)
head(pol_final)
head(claims_final)
#' ## Do some sense checking
#'
#' You should always check that you didn't lose or make up any data
#' especially when doing joins.
# Next two statements need to output the same number.
sum(pol_final$claim_count)
nrow(claims)
# Next three statements need to output the same number.
sum(pol_final$total_ultimate)
sum(claims$claim_ultimate)
sum(pol_final$average_severity * pol_final$claim_count)
# Next three statements need to output the same number.
nrow(pol_final)
length(unique(pol_final$policy_number))
length(unique(pol$policy_number))
#' ## Save files
#'
#' I'm going to be fancy, because I want a timestamp in the file name.
fname <- paste0(
'c:/home/git/other/ratemaking-capstone/demo/data-',
format(Sys.time(), '%Y-%m-%d-%H%M'),
'.RData')
print(fname)
save(pol_final, claims_final, file = fname)
read('./share/claims.RData')
load('./share/claims.RData')
load('./share/pol_dates.RData.RData')
load('./share/pol_rating.RData.RData')
load('./share/pol_dates.RData')
load('./share/pol_rating.RData')
ls()
write.csv(claims, './share/claims.csv')
write.csv(pol_dates, './share/pol_dates.csv')
write.csv(pol_rating, './share/pol_rating.csv')
dim(pol_rating)
