#' ---
#' title:  "Ratemaking Capstone v1"
#' author: "ALR"
#' date:   "July 24, 2018"
#' output: html_document
#' ---
#+ include=FALSE
# knitr::opts_chunk$set(echo = FALSE)
require(alrtools)
require(magrittr)
require(dplyr)
require(ChainLadder)
require(tree)
#+ include=FALSE
# Description:
#   Test of whether the capstone works!
#   This is an R file with comments using RMarkdown syntax
#
#+ include=FALSE
getwd()
dir('./share', pattern = 'RData')
#+ include=FALSE
# Load all the data
load('./share/claims.RData')
load('./share/pol_dates.RData')
load('./share/pol_rating.RData')
#+ include=FALSE
# Are there closed claims with 0 payment?
claims %>%
filter(status == 'C', claim_ultimate < 1)
summary(claims)
# Get average severity
avg_severity <- claims %>%
mutate(year = floor(claim_made / 100)) %>%
filter(status == 'C') %>%
group_by(year) %>%
summarize(count = n(), severity = sum(claim_ultimate)) %>%
mutate(avg_severity = severity / count)
lm_as <- lm(log(avg_severity) ~ year, data = avg_severity)
own_trend <- lm_as$coefficients[2] %>% exp - 1
industry_trend <- 0.03
claims %>% head
claims$cm_year <- floor(claims$claim_made / 100)
claims %>% head
#' ## Headers for each object
names(pol_dates)
names(pol_rating)
names(pol_rating)
names(claims)
#' Are the training and testing sets mutually exclusive?
pols <- unique(pol_dates$policy_number)
pols %>% length
pol_dates %>% nrow
pols <- unique(pol_dates$policy_number)
pols %>% length
pol_dates %>% nrow
sum(pols %in% pols)
sum(pols %in% pols)
pols <- unique(pol_rating$policy_number)
pols %>% length
pols <- unique(pol_rating$policy_number)
pols %>% length
sum(pols %in% pols)
sum(pols %in% pols)
#' Are status and status.1 the same?  If so, remove status.1
sum(!claims$status == claims$status.1)
claims$status.1 <- NULL
#' pols3 objects have multiple rows per policy.
#' What is the key on this table?
#' Check that it is policy_number and variable
nrow(pol_rating[, c('policy_number', 'variable')])
nrow(unique(pol_rating[, c('policy_number', 'variable')]))
#' Since these have the same number of rows
#' then policy_number, variable is a key.
#' We need to un-melt the pol_rating object and join it with the pol_dates object.
polw <- tidyr::spread(pol_rating, variable, value)
pol <- merge(pol_dates, polw)
#' Did the merge work?
nrow(pol)
sum(pol$policy_number %in% pols)
#' Some of the data need to be converted.
pol$inception <- as.numeric(pol$inception)
pol$expiration <- as.numeric(pol$expiration)
pol$revenue <- as.numeric(pol$revenue)
pol$five_year_claims <- as.numeric(pol$five_year_claims)
pol$employee_count <- as.numeric(pol$employee_count)
#' We now need to calculate trend in our loss data, if it has any.
#' It might be best to create a one-way table generator first.
#' Let's first add current incurred to the policy table.
inc <- claims %>% group_by(policy_number) %>%
summarize(
total_incurred = sum(claim_ultimate),
claim_counts = n()
)
pol_inc <- merge(pol, inc, all.x = TRUE)
pol_inc$total_incurred[is.na(pol_inc$total_incurred)] <- 0
pol_inc$claim_counts[is.na(pol_inc$claim_counts)] <- 0
sum(pol_inc$total_incurred)
sum(claims$claim_ultimate)
sum(pol_inc$claim_counts)
nrow(claims)
pol_inc$py <- left(pol_inc$inception, 4) %>% as.numeric
pol_inc %>% group_by(py) %>%
summarize(
pol_counts = n(),
claim_counts = sum(claim_counts),
total_incurred = sum(total_incurred),
revenue = sum(revenue),
employee_count = sum(employee_count),
five_year_claims = sum(five_year_claims)
) %>% as.data.frame
# Get policy data in claims table
claims <- merge(claims, pol)
claims$policy_year <- floor(as.numeric(claims$inception) / 100)
claims_py <- claims %>% group_by(policy_year, status) %>%
summarize(count = n(), inc = sum(claim_ultimate))
log(claims$claim_ultimate) %>% hist
sum(pol_inc$claim_counts)
nrow(claims)
mean(pol_inc$claim_counts)
sd(pol_inc$claim_counts)
# Get state group
state <- read.csv('./data/states.csv', stringsAsFactors = TRUE)
state_levels <- state$State
pstates <- factor(pol_inc$state, levels = state_levels)
pol_inc$StateGroup <- state$Frequency.Group[pstates]
model1 <- glm(claim_counts ~ revenue + StateGroup + discipline, data = pol_inc)
summary(model1)
tree(claim_counts, data = pol_inc)
tbos <- tree(
formula = claim_counts ~ revenue + StateGroup + discipline,
data = pol_inc
)
tbos <- tree(
formula = claim_counts ~ revenue,
data = pol_inc
)
tbos
plot(tbos)
tpol <- tree(
formula = claim_counts ~ revenue,
data = pol_inc
)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
tpol <- tree(
formula = claim_counts ~ revenue + StateGroup,
data = pol_inc
)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
tpol <- tree(
formula = claim_counts ~ revenue + StateGroup + discipline,
data = pol_inc
)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
plot(tpol)
rm(tpol)
tpol <- tree(
formula = claim_counts ~ revenue + discipline,
data = pol_inc
)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
#' ---
#' title:  "Ratemaking Capstone v1"
#' author: "ALR"
#' date:   "July 24, 2018"
#' output: html_document
#' ---
#+ include=FALSE
# knitr::opts_chunk$set(echo = FALSE)
require(alrtools)
require(magrittr)
require(dplyr)
require(ChainLadder)
require(tree)
#+ include=FALSE
# Description:
#   Test of whether the capstone works!
#   This is an R file with comments using RMarkdown syntax
#
#+ include=FALSE
getwd()
dir('./share', pattern = 'RData')
#+ include=FALSE
# Load all the data
load('./share/claims.RData')
load('./share/pol_dates.RData')
load('./share/pol_rating.RData')
#+ include=FALSE
# Are there closed claims with 0 payment?
claims %>%
filter(status == 'C', claim_ultimate < 1)
summary(claims)
# Get average severity
avg_severity <- claims %>%
mutate(year = floor(claim_made / 100)) %>%
filter(status == 'C') %>%
group_by(year) %>%
summarize(count = n(), severity = sum(claim_ultimate)) %>%
mutate(avg_severity = severity / count)
lm_as <- lm(log(avg_severity) ~ year, data = avg_severity)
own_trend <- lm_as$coefficients[2] %>% exp - 1
industry_trend <- 0.03
claims %>% head
claims$cm_year <- floor(claims$claim_made / 100)
claims %>% head
#' ## Headers for each object
names(pol_dates)
names(pol_rating)
names(pol_rating)
names(claims)
#' Are the training and testing sets mutually exclusive?
pols <- unique(pol_dates$policy_number)
pols %>% length
pol_dates %>% nrow
pols <- unique(pol_dates$policy_number)
pols %>% length
pol_dates %>% nrow
sum(pols %in% pols)
sum(pols %in% pols)
pols <- unique(pol_rating$policy_number)
pols %>% length
pols <- unique(pol_rating$policy_number)
pols %>% length
sum(pols %in% pols)
sum(pols %in% pols)
#' Are status and status.1 the same?  If so, remove status.1
sum(!claims$status == claims$status.1)
claims$status.1 <- NULL
#' pols3 objects have multiple rows per policy.
#' What is the key on this table?
#' Check that it is policy_number and variable
nrow(pol_rating[, c('policy_number', 'variable')])
nrow(unique(pol_rating[, c('policy_number', 'variable')]))
#' Since these have the same number of rows
#' then policy_number, variable is a key.
#' We need to un-melt the pol_rating object and join it with the pol_dates object.
polw <- tidyr::spread(pol_rating, variable, value)
pol <- merge(pol_dates, polw)
#' Did the merge work?
nrow(pol)
sum(pol$policy_number %in% pols)
#' Some of the data need to be converted.
pol$inception <- as.numeric(pol$inception)
pol$expiration <- as.numeric(pol$expiration)
pol$revenue <- as.numeric(pol$revenue)
pol$five_year_claims <- as.numeric(pol$five_year_claims)
pol$employee_count <- as.numeric(pol$employee_count)
#' We now need to calculate trend in our loss data, if it has any.
#' It might be best to create a one-way table generator first.
#' Let's first add current incurred to the policy table.
inc <- claims %>% group_by(policy_number) %>%
summarize(
total_incurred = sum(claim_ultimate),
claim_counts = n()
)
pol_inc <- merge(pol, inc, all.x = TRUE)
pol_inc$total_incurred[is.na(pol_inc$total_incurred)] <- 0
pol_inc$claim_counts[is.na(pol_inc$claim_counts)] <- 0
sum(pol_inc$total_incurred)
sum(claims$claim_ultimate)
sum(pol_inc$claim_counts)
nrow(claims)
pol_inc$py <- left(pol_inc$inception, 4) %>% as.numeric
pol_inc %>% group_by(py) %>%
summarize(
pol_counts = n(),
claim_counts = sum(claim_counts),
total_incurred = sum(total_incurred),
revenue = sum(revenue),
employee_count = sum(employee_count),
five_year_claims = sum(five_year_claims)
) %>% as.data.frame
# Get policy data in claims table
claims <- merge(claims, pol)
claims$policy_year <- floor(as.numeric(claims$inception) / 100)
claims_py <- claims %>% group_by(policy_year, status) %>%
summarize(count = n(), inc = sum(claim_ultimate))
log(claims$claim_ultimate) %>% hist
sum(pol_inc$claim_counts)
nrow(claims)
mean(pol_inc$claim_counts)
sd(pol_inc$claim_counts)
# Get state group
state <- read.csv('./data/states.csv', stringsAsFactors = TRUE)
state_levels <- state$State
pstates <- factor(pol_inc$state, levels = state_levels)
pol_inc$StateGroup <- state$Frequency.Group[pstates]
model1 <- glm(claim_counts ~ revenue + StateGroup + discipline, data = pol_inc)
summary(model1)
tpol <- tree(
formula = claim_counts ~ revenue + discipline,
data = pol_inc
)
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
load("claims.RData")
require(devtools)
require(magrittr)
require(rmarkdown)
require(knitr)
require(alrtools)
require(MASS)
require(reshape2)
source('./R/resources.R')
val_date <- 201612
#############################################
# POLICIES
#   Line is A&E
#   Exposure base is revenue
#
#   Frequency of claim is a function of
#     Region
#     Revenue
#     Primary Discipline
#
#   Also include
#     Number of Architects
#     Whether they use written contracts
#     Longevity in business
#     Number of Claims in Last Five Years
#
#############################################
# Number of policies
n <- 1e5
policies <- data.frame(index = 1:n)
set.seed(912387)
policies$policy_number <- paste0(
'C1AE', right(paste0('00000000', sample(1:1e6, n)), 8))
policies$policy_year <- sample(
x = 2007:2016,
size = n,
prob = c(1, 2, 3, 4, 5, 5, 5, 5, 5, 5),
replace = TRUE
)
policies$duration_months <- 12
policies$policy_month <- sample(1:12, n, replace = TRUE)
policies$inception <- paste0(
policies$policy_year,
right(paste0('00', policies$policy_month), 2)
)
a <- (policies$policy_month + policies$duration_months)
b <- floor((a - 1) / 12)
policies$expiration <- paste0(
policies$policy_year + b,
right(paste0('00', a - 12*b), 2)
)
# Revenue
#   Minimum revenue should be 50e3
#   Maximum should be 5e6
revenue_low   <- c(50e3, 100e3, 250e3, 500e3, 1e6, 2.5e6)
revenue_high  <- c(revenue_low[-1], 5e6)
n_rev         <- 6
policies$revenue_bucket <- sample(
x = 1:6,
size = n,
prob = c(6, 5, 4, 3, 2, 1),
replace = TRUE
)
a <- runif(n)
b <- policies$revenue_bucket
policies$revenue <- round((revenue_high[b] - revenue_low[b]) * a +
revenue_low[b], 0)
# State/Region
#   Use CSV table for groupings
states <- read.csv('./data/states.csv', stringsAsFactors = FALSE)
a <- sample(1:nrow(states), n, prob = states$Population, replace = TRUE)
policies$state <- states$State[a]
policies$state_group <- states$Frequency.Group[a]
a <- factor(policies$state_group, levels = c('Low', 'Mid', 'High'))
policies$state_relativity <- c(1.0, 1.25, 1.5)[as.integer(a)]
# If revenue is over $4m state does not matter
policies$state_relativity[policies$revenue >= 4e6] <- 3.5
# Disciplines
#  Use CSV table for relativities
disciplines <- read.csv('./data/disciplines.csv', stringsAsFactors = FALSE)
a <- sample(
x = 1:nrow(disciplines),
size = n,
prob = disciplines$Probability,
replace = TRUE
)
policies$discipline <- disciplines$Discipline[a]
policies$discipline_relativity <- disciplines$Relativity[a]
policies$discipline_group <- paste0('d', policies$discipline_relativity)
# Revenue frequency
#   Expected is 0.05 per million in claims
#   Remember to adjust rev as it is annual!
policies$revenue_frequency <- policies$revenue / 1e6 *
0.05 * policies$duration_months / 12
policies$expected_frequency <-
policies$revenue_frequency * policies$discipline_relativity *
policies$state_relativity
# Get expected claim counts
odf <- 1.5
policies$claim_count <- rnegbin(
n = n,
mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1)
)
# #############################################
# # GLM TEST for FREQ
# # TODO Need to flesh this out
# #############################################
#
# glm_freq <- glm(
#   data = policies,
#   formula = claim_count ~
#     state_group + discipline_group + revenue + duration_months,
#   family = quasipoisson
# )
#
# summary(glm_freq)
#############################################
# CLAIMS
#############################################
# Number of claims total
m <- sum(policies$claim_count)
claims <- data.frame(claimindex = 1:m)
# Merge with policies table
lpolicies <- policies[policies$claim_count > 0, ]
a <- cumsum(lpolicies$claim_count)
b <- c(1, a[-length(a)] + 1)
lpolicies$claimindex_start <- b
claims <- cbind(claims, lookup(claims, lpolicies))
# Claim made date
#   Select a month at random from the duration
a <- floor(runif(m) * (claims$duration_months + 3))
claims$claim_made <- add_yyyymm(claims$inception, a)
# Claim closed date
#   Expect duration to be 2 years from report
#   Most between 1 and 3, so normal with s.d. = 0.5, mu = 2
a <- round(rnorm(m, mean = 24, sd = 6), 0)
a[a < 0] <- 0
claims$claim_closed <- add_yyyymm(claims$claim_made, a)
# Claim status
# Valuation date is val_date
claims$status <- ifelse(claims$claim_closed <= val_date, 'C', 'O')
# Fixed to use claim made date instead of policy date for trend
claims$claim_made_year <- year_yyyymm(claims$claim_made)
# Do some claims testing
#############################################
# CLAIM SEVERITY
#   Average severity in 2016 will be $95k
#   Expect claims inflation of 3%
#   Model ultimate values using lognormal
#############################################
years              <- min(claims$claim_made_year):max(claims$claim_made_year)
inflation          <- 0.00
n_years            <- length(years)
inflation_factors  <- (1 + inflation) ^ (years - 2016)
avg_severity       <- inflation_factors * 95000
# lognormal mean
#
#   E[X] = exp(mu + sigma^2/2)
#   log(E[x]) = mu + sigma^2/2
#   mu = log(E[x]) - sigma^2/2
#
# Pick sigma as 1.4 and keep constant for each year
sigma <- rep(1.0, length(avg_severity))
mu <- log(avg_severity) - sigma^2/2
lparam <- data.frame(
claim_made_year = years,
sev_mu = mu,
sev_sigma = sigma
)
# Ultimate claim value
claims$sev_mu <- NULL
claims$sev_sigma <- NULL
a <- lookup(claims, lparam)
claims$sev_mu <- a$sev_mu
claims$sev_sigma <- a$sev_sigma
claims$claim_ultimate <- rlnorm(m, claims$sev_mu, claims$sev_sigma)
claims$count <- 1
# [August 21, 2017 ALR]
# Make sure that the claim trend is calculatable
avg_severity <- aggregate(
x = claims[, c('claim_ultimate', 'count')],
by = claims[, 'claim_made_year', drop = FALSE],
FUN = sum
)
avg_severity$avg_severity <-
avg_severity$claim_ultimate / avg_severity$count
#############################################
# Add some unimportant data to policies
#############################################
policies$year_started <-
policies$policy_year - sample(0:20, size = n, replace = TRUE)
a <- rnorm(n, 100000, 50000)
a[a < 50000] <- 50000
policies$employee_count <- floor(policies$revenue / a) + 1
policies$use_written_contracts <-
sample(c('Y', 'N'), n, prob = c(0.8, 0.2), replace = TRUE)
policies$five_year_claims <-
rnegbin(n = n, mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1)) +
rnegbin(n = n, mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1)) +
rnegbin(n = n, mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1)) +
rnegbin(n = n, mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1)) +
rnegbin(n = n, mu = policies$expected_frequency,
theta = policies$expected_frequency / (odf - 1))
policies$five_year_claims <-
floor(pmin(5, 2016 - policies$year_started) *
policies$five_year_claims / 5)
